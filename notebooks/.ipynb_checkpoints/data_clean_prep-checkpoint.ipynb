{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from googlemaps import Client as GoogleMaps\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "\n",
    "import shapefile as shp\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "from utils import * \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and cleaning data  \n",
    "\n",
    "1. Lyme disease per county per year (https://www.cdc.gov/lyme/datasurveillance/index.html)\n",
    "\n",
    "2. County locations and shapefile (http://eric.clst.org/tech/usgeojson/)\n",
    "\n",
    "3. Temperature and precipitation data per month per county (ftp://ftp.ncdc.noaa.gov/pub/data/cirs/climdiv/)\n",
    "\n",
    "4. Deer population by county (https://data.nal.usda.gov/dataset/white-tailed-deer-density-estimates-across-eastern-united-states-2008)\n",
    "\n",
    "     - deer_density_QDMA.zip = GIS shapefile depicting white-tailed deer density summarized between 2001 and 2005 by the Quality Deer Management Association for the eastern United States. Categories represent coarse deer density levels as identified in the QDMA report in 2009: (1) rare, absent, or urban area with unknown population, (2) less than 15 deer per square mile, (3) 15 to 30 deer per square mile, (4) 30 to 40 deer per square mile, or (5) greater than 45 deer per square mile. Deer density estimates represent levels summarized between 2001 and 2005 and should not be used to represent current or future deer density levels.\n",
    "\n",
    "5. Forest cover data \n",
    "\n",
    "6. Climate projections for the same resolution in time/space (https://cida.usgs.gov/gdp/client/#!catalog/gdp/dataset/54dd5e4be4b08de9379b38ff)\n",
    "    - data obtained, need to clean up DFs \n",
    "    \n",
    "7. Add dog data if we can get it : https://capcvet.org/maps/#2017/all/lyme-disease/dog/canada/\n",
    "8. Need the population per year of counties! \n",
    "9. https://www.health.ny.gov/statistics/diseases/communicable/ for NY Lyme numbers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lyme/county data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_shapefile = gpd.read_file(\"./data/raw/gz_2010_us_050_00_20m/gz_2010_us_050_00_20m.shp\")\n",
    "lyme_per_county = pd.read_csv('./data/raw/LD-Case-Counts-by-County-00-17.csv',encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## making the labelling of counties compatible in both files \n",
    "## 3,007 counties, 64 parishes, 19 organized boroughs, 10 census areas, 41 independent cities, and the District of Columbia = 3142 total.\n",
    "\n",
    "county_shapefile['long_lat'] = county_shapefile['geometry'].apply(lambda x : list(x.centroid.coords)[0])\n",
    "county_shapefile['LSAD'] = county_shapefile['LSAD'].replace('CA', 'Census Area') \n",
    "county_shapefile['LSAD'] = county_shapefile['LSAD'].replace('Cty&Bor', 'City and Borough') \n",
    "county_shapefile['LSAD'] = county_shapefile['LSAD'].replace('Muny', 'Municipality') \n",
    "county_shapefile.iloc[777, county_shapefile.columns.get_loc('LSAD')] = ''\n",
    "county_shapefile.iloc[1107, county_shapefile.columns.get_loc('LSAD')] = ''\n",
    "\n",
    "county_shapefile['NEWNAME'] = county_shapefile['NAME'] + ' ' + county_shapefile['LSAD']\n",
    "\n",
    "county_shapefile.COUNTY = county_shapefile.COUNTY.astype('int')\n",
    "county_shapefile.STATE = county_shapefile.STATE.astype('int')\n",
    "\n",
    "lyme_per_county = lyme_per_county[lyme_per_county['CTYCODE']!=999]\n",
    "county_shapefile = county_shapefile[county_shapefile['STATE']!=72]\n",
    "\n",
    "assert len(lyme_per_county)==len(county_shapefile)\n",
    "merged = pd.merge(lyme_per_county, county_shapefile,  how='left', left_on=['CTYCODE', 'STCODE'], right_on = ['COUNTY', 'STATE'])\n",
    "assert len(merged)==len(lyme_per_county)==len(county_shapefile)\n",
    "\n",
    "merged[['Ctyname', 'Stname', 'STCODE', 'CTYCODE']].to_csv('./data/interim/all_county_state.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ctyname</th>\n",
       "      <th>Stname</th>\n",
       "      <th>STCODE</th>\n",
       "      <th>CTYCODE</th>\n",
       "      <th>Cases2000</th>\n",
       "      <th>Cases2001</th>\n",
       "      <th>Cases2002</th>\n",
       "      <th>Cases2003</th>\n",
       "      <th>Cases2004</th>\n",
       "      <th>Cases2005</th>\n",
       "      <th>...</th>\n",
       "      <th>Cases2017</th>\n",
       "      <th>GEO_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LSAD</th>\n",
       "      <th>CENSUSAREA</th>\n",
       "      <th>geometry</th>\n",
       "      <th>long_lat</th>\n",
       "      <th>NEWNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Kusilvak Census Area</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>84</td>\n",
       "      <td>0500000US11001</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>None</td>\n",
       "      <td>61.048</td>\n",
       "      <td>POLYGON ((-77.0329858478747 38.839500154093, -...</td>\n",
       "      <td>(-77.01578736260959, 38.90565503863293)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>Carson City</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>32</td>\n",
       "      <td>510</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0500000US32510</td>\n",
       "      <td>32.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>Carson City</td>\n",
       "      <td>None</td>\n",
       "      <td>144.662</td>\n",
       "      <td>POLYGON ((-120.004504420333 39.1655986798664, ...</td>\n",
       "      <td>(-119.74298726381755, 39.15066857645598)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>Oglala Lakota County</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Ctyname                Stname  STCODE  CTYCODE  Cases2000  \\\n",
       "81    Kusilvak Census Area                Alaska       2      158          0   \n",
       "319   District of Columbia  District of Columbia      11        1         11   \n",
       "1763           Carson City                Nevada      32      510          0   \n",
       "2412  Oglala Lakota County          South Dakota      46      102          0   \n",
       "\n",
       "      Cases2001  Cases2002  Cases2003  Cases2004  Cases2005  ...  Cases2017  \\\n",
       "81            0          0          0          0          0  ...          0   \n",
       "319          17         25         14         16         10  ...         84   \n",
       "1763          1          0          0          0          0  ...          2   \n",
       "2412          0          0          0          0          0  ...          0   \n",
       "\n",
       "              GEO_ID  STATE  COUNTY                  NAME  LSAD  CENSUSAREA  \\\n",
       "81               NaN    NaN     NaN                   NaN   NaN         NaN   \n",
       "319   0500000US11001   11.0     1.0  District of Columbia  None      61.048   \n",
       "1763  0500000US32510   32.0   510.0           Carson City  None     144.662   \n",
       "2412             NaN    NaN     NaN                   NaN   NaN         NaN   \n",
       "\n",
       "                                               geometry  \\\n",
       "81                                                  NaN   \n",
       "319   POLYGON ((-77.0329858478747 38.839500154093, -...   \n",
       "1763  POLYGON ((-120.004504420333 39.1655986798664, ...   \n",
       "2412                                                NaN   \n",
       "\n",
       "                                      long_lat  NEWNAME  \n",
       "81                                         NaN      NaN  \n",
       "319    (-77.01578736260959, 38.90565503863293)      NaN  \n",
       "1763  (-119.74298726381755, 39.15066857645598)      NaN  \n",
       "2412                                       NaN      NaN  \n",
       "\n",
       "[4 rows x 31 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting oglala and all of Alaska \n",
    "\n",
    "merged = merged.drop(merged[merged['Stname'] == 'Hawaii'].index)\n",
    "merged = merged.drop(merged[merged['Stname'] == 'Alaska'].index)\n",
    "merged = merged.drop(merged[merged['Ctyname'] == 'Oglala Lakota County'].index)\n",
    "\n",
    "# no nore nans merged[merged.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the temp and precipitation data from the NCDC\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## file format \n",
    "\n",
    "ST DIV ID YEAR  JAN    FEB    MAR    APR    MAY    JUNE   JULY   AUG    SEP    OCT    NOV    DEC  \n",
    "01 001 02 1895  44.00  38.20  55.50  64.10  70.60  78.30  80.40  80.40  79.00  61.40  54.40  45.30\n",
    "\n",
    "50 290 27 2019   5.40  17.40  33.20  38.60  57.10 -99.90 -99.90 -99.90 -99.90 -99.90 -99.90 -99.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "colspecs = [[0,2],[2,5],[5,7],[7,11],[11,18],[18,25], [25,32], [32,39],[39,46],[46,53],[53,60],[60,67],[67,74],[74,81],[81,88],[88,95]]\n",
    "tmax_data = pd.read_fwf('./data/raw/clim/climdiv-tmaxcy-v1.0.0-20190604', index_col=None, colspecs=colspecs, header=None)\n",
    "tmin_data = pd.read_fwf('./data/raw/clim/climdiv-tmincy-v1.0.0-20190604', index_col=None, colspecs=colspecs, header=None)\n",
    "tmpc_data = pd.read_fwf('./data/raw/clim/climdiv-tmpccy-v1.0.0-20190604', index_col=None, colspecs=colspecs, header=None)\n",
    "pcpn_data = pd.read_fwf('./data/raw/clim/climdiv-pcpncy-v1.0.0-20190604', index_col=None, colspecs=colspecs, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsuffixes = ['_TMAX', '_TMIN', '_TMPC', '_PCPN']\n",
    "dfs = [tmax_data, tmin_data, tmpc_data, pcpn_data]\n",
    "\n",
    "for i in range(4):\n",
    "    df = dfs[i]\n",
    "    df.columns = ['ST', 'DIV', 'EL', 'YR', 'JAN'+wsuffixes[i], 'FEB'+wsuffixes[i], 'MAR'+wsuffixes[i], 'APR'+wsuffixes[i], 'MAY'+wsuffixes[i], 'JUN'+wsuffixes[i], 'JUL'+wsuffixes[i], 'AUG'+wsuffixes[i], 'SEP'+wsuffixes[i], 'OCT'+wsuffixes[i], 'NOV'+wsuffixes[i], 'DEC'+wsuffixes[i]]\n",
    "    \n",
    "tmin_data = tmin_data[tmin_data['YR']>=1998]\n",
    "tmax_data = tmax_data[tmax_data['YR']>=1998]\n",
    "tmpc_data = tmpc_data[tmpc_data['YR']>=1998]\n",
    "pcpn_data = pcpn_data[pcpn_data['YR']>=1998]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid_codes = pd.read_csv('./data/raw/state_code_stupid.csv')\n",
    "logical_codes = pd.read_csv('./data/raw/states_code_logical.csv')\n",
    "stupid_codes.name = stupid_codes.name.str.lower().str.strip()\n",
    "logical_codes.name = logical_codes.name.str.lower().str.strip()\n",
    "stupid_codes.columns=['code','num_stupid','name_stupid']\n",
    "code_mapping = logical_codes.set_index('name').join(stupid_codes.set_index('name_stupid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all the files together \n",
    "\n",
    "tmin_tmax = pd.merge(tmin_data, tmax_data ,how='left', left_on=['ST', 'DIV', 'YR'], right_on = ['ST', 'DIV', 'YR'])\n",
    "tmin_tmax = tmin_tmax.loc[:,~tmin_tmax.columns.duplicated()]\n",
    "tmpc_tmin_tmax = pd.merge(tmpc_data, tmin_tmax ,how='left', left_on=['ST', 'DIV', 'YR'], right_on = ['ST', 'DIV', 'YR'])\n",
    "tmpc_tmin_tmax = tmpc_tmin_tmax.loc[:,~tmpc_tmin_tmax.columns.duplicated()]\n",
    "pcpn_tmpc_tmin_tmax = pd.merge(pcpn_data, tmpc_tmin_tmax ,how='left', left_on=['ST', 'DIV', 'YR'], right_on = ['ST', 'DIV', 'YR'])\n",
    "pcpn_tmpc_tmin_tmax = pcpn_tmpc_tmin_tmax.loc[:,~pcpn_tmpc_tmin_tmax.columns.duplicated()]\n",
    "\n",
    "assert len(pcpn_tmpc_tmin_tmax)==len(tmin_tmax)\n",
    "\n",
    "pcpn_tmpc_tmin_tmax = pcpn_tmpc_tmin_tmax[pcpn_tmpc_tmin_tmax['ST']!=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacing the 1-48 numbering by the FIPS codes for the states \n",
    "# there's no precipitation/temp data for DC, so we should remove that from merged \n",
    "\n",
    "pcpn_tmpc_tmin_tmax['ST_new'] = pcpn_tmpc_tmin_tmax['ST'].apply(lambda x: replace_st_code(x))\n",
    "merged = merged.drop(merged[merged['STCODE'] == 11].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcpn_tmpc_tmin_tmax.to_csv('./data/interim/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring how to merge all the data \n",
    "finaldf = None\n",
    "\n",
    "assert len(pcpn_tmpc_tmin_tmax['ST_new'].unique())==len(merged['STCODE'].unique())\n",
    "\n",
    "for year in range(2000,2018): \n",
    "    temp_lyme = merged[['Ctyname','Stname', 'STCODE', 'CTYCODE',  'Cases'+str(year), 'CENSUSAREA', 'geometry','long_lat']]\n",
    "    temp_tpc = pcpn_tmpc_tmin_tmax[pcpn_tmpc_tmin_tmax['YR']==year]\n",
    "    temp_merged = pd.merge(temp_lyme, temp_tpc ,how='left', left_on=['STCODE', 'CTYCODE'], right_on = ['ST_new','DIV'])\n",
    "    temp_merged.rename(columns={'Cases'+str(year): 'Cases'}, inplace=True)\n",
    "    temp_merged['year'] = int(year)\n",
    "\n",
    "    if finaldf is None: \n",
    "        finaldf = temp_merged\n",
    "    else:\n",
    "        finaldf = finaldf.append(temp_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55890, 66)\n"
     ]
    }
   ],
   "source": [
    "# the weather data is missing a couple of counties from Virginia, so dropping them. \n",
    "\n",
    "#pcpn_tmpc_tmin_tmax[pcpn_tmpc_tmin_tmax['DIV']==678]\n",
    "#pcpn_tmpc_tmin_tmax[pcpn_tmpc_tmin_tmax['DIV']==515]\n",
    "\n",
    "finaldf = finaldf.dropna(axis=0, how='any')\n",
    "finaldf['State FIPS Code'] = finaldf['STCODE'].apply(lambda x: str(x).zfill(2))\n",
    "finaldf['County FIPS Code'] = finaldf['CTYCODE'].apply(lambda x: str(x).zfill(3))\n",
    "finaldf['FIPS'] = finaldf['State FIPS Code'] + finaldf['County FIPS Code']\n",
    "finaldf.to_csv('./data/interim/lyme_weather.csv')\n",
    "print(finaldf.shape)\n",
    "\n",
    "#finaldf[finaldf.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest cover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('./data/raw/ncld_states/gaplf2011lc_v30_AL/GAP_LANDFIRE_National_Terrestrial_Ecosystems_2011_Attributes.txt',sep='\\t')\n",
    "attr_val = attributes[['Value','NVC_CLASS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf.drop(finaldf[finaldf['Stname'] == 'Hawaii'].index)\n",
    "finaldf = finaldf.drop(finaldf[finaldf['Stname'] == 'Alaska'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['FIPS'] +  list(attr_val.NVC_CLASS.unique())[1:] \n",
    "list_fips = finaldf['FIPS'].unique()\n",
    "coverdf =  pd.DataFrame(columns=list_cols)\n",
    "\n",
    "for i in range(3105):\n",
    "    \n",
    "    ele = list_fips[i]\n",
    "    forest = pd.read_csv('./data/county_cover/'+str(ele)+'.csv')\n",
    "    temp = forest.set_index('myclass').join(attr_val.set_index('Value'))\n",
    "    temp = temp.reset_index()\n",
    "    temp.columns= ['class', 'index', 'fraction', 'nvc_class']\n",
    "    temp = temp.groupby('nvc_class').sum()\n",
    "    temp = temp.reset_index()\n",
    "    temp = temp.drop(labels=['index', 'class'], axis=1)\n",
    "    temp.set_index('nvc_class', inplace=True)\n",
    "    temp = temp.transpose()\n",
    "\n",
    "    newdf = pd.DataFrame(columns=list_cols)\n",
    "    for col in newdf.columns[1:]:\n",
    "        if col in temp.columns:\n",
    "            newdf[col] = temp[col]\n",
    "        else:\n",
    "            newdf[col] = 1e-10\n",
    "    newdf.index = [0]    \n",
    "    newdf.at[0,'FIPS'] = ele\n",
    "\n",
    "    coverdf = coverdf.append(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Forest &amp; Woodland</th>\n",
       "      <th>Shrub &amp; Herb Vegetation</th>\n",
       "      <th>Desert &amp; Semi-Desert</th>\n",
       "      <th>Polar &amp; High Montane Scrub, Grassland &amp; Barrens</th>\n",
       "      <th>Aquatic Vegetation</th>\n",
       "      <th>Open Rock Vegetation</th>\n",
       "      <th>Nonvascular &amp; Sparse Vascular Rock Vegetation</th>\n",
       "      <th>Agricultural &amp; Developed Vegetation</th>\n",
       "      <th>Introduced &amp; Semi Natural Vegetation</th>\n",
       "      <th>Recently Disturbed or Modified</th>\n",
       "      <th>Open Water</th>\n",
       "      <th>Developed &amp; Other Human Use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>0.685407</td>\n",
       "      <td>0.231207</td>\n",
       "      <td>0.031586</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.017688</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.895136e-03</td>\n",
       "      <td>6.317119e-04</td>\n",
       "      <td>1.895136e-03</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01003</td>\n",
       "      <td>0.524983</td>\n",
       "      <td>0.286790</td>\n",
       "      <td>0.064339</td>\n",
       "      <td>0.015514</td>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.045175</td>\n",
       "      <td>6.160164e-03</td>\n",
       "      <td>8.898015e-03</td>\n",
       "      <td>5.475702e-03</td>\n",
       "      <td>5.932010e-03</td>\n",
       "      <td>5.932010e-03</td>\n",
       "      <td>2.281542e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01005</td>\n",
       "      <td>0.756405</td>\n",
       "      <td>0.188576</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>4.199916e-04</td>\n",
       "      <td>2.519950e-03</td>\n",
       "      <td>4.619908e-03</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01007</td>\n",
       "      <td>0.825015</td>\n",
       "      <td>0.149729</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01009</td>\n",
       "      <td>0.373501</td>\n",
       "      <td>0.483724</td>\n",
       "      <td>0.075385</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.005140</td>\n",
       "      <td>0.023986</td>\n",
       "      <td>5.711022e-04</td>\n",
       "      <td>1.142204e-03</td>\n",
       "      <td>1.713307e-03</td>\n",
       "      <td>1.142204e-03</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIPS  Forest & Woodland  Shrub & Herb Vegetation  Desert & Semi-Desert  \\\n",
       "0  01001           0.685407                 0.231207              0.031586   \n",
       "0  01003           0.524983                 0.286790              0.064339   \n",
       "0  01005           0.756405                 0.188576              0.014700   \n",
       "0  01007           0.825015                 0.149729              0.008419   \n",
       "0  01009           0.373501                 0.483724              0.075385   \n",
       "\n",
       "   Polar & High Montane Scrub, Grassland & Barrens  Aquatic Vegetation  \\\n",
       "0                                         0.003790            0.000632   \n",
       "0                                         0.015514            0.008898   \n",
       "0                                         0.002940            0.001680   \n",
       "0                                         0.002405            0.000601   \n",
       "0                                         0.010280            0.005140   \n",
       "\n",
       "   Open Rock Vegetation  Nonvascular & Sparse Vascular Rock Vegetation  \\\n",
       "0              0.017688                                   1.000000e-10   \n",
       "0              0.045175                                   6.160164e-03   \n",
       "0              0.007980                                   1.000000e-10   \n",
       "0              0.000601                                   1.000000e-10   \n",
       "0              0.023986                                   5.711022e-04   \n",
       "\n",
       "   Agricultural & Developed Vegetation  Introduced & Semi Natural Vegetation  \\\n",
       "0                         1.895136e-03                          6.317119e-04   \n",
       "0                         8.898015e-03                          5.475702e-03   \n",
       "0                         1.000000e-10                          4.199916e-04   \n",
       "0                         1.000000e-10                          1.000000e-10   \n",
       "0                         1.142204e-03                          1.713307e-03   \n",
       "\n",
       "   Recently Disturbed or Modified    Open Water  Developed & Other Human Use  \n",
       "0                    1.895136e-03  1.000000e-10                 1.000000e-10  \n",
       "0                    5.932010e-03  5.932010e-03                 2.281542e-04  \n",
       "0                    2.519950e-03  4.619908e-03                 1.000000e-10  \n",
       "0                    1.000000e-10  1.000000e-10                 1.000000e-10  \n",
       "0                    1.142204e-03  1.000000e-10                 1.000000e-10  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverdf.to_csv('./data/interim/county_cover_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyme_weather_cover = finaldf.set_index(keys='FIPS').join(coverdf.set_index(keys='FIPS'))\n",
    "\n",
    "# there's 47 values missing from coverdf, which result in 846 missing values/nans for lyme_weather_cover\n",
    "\n",
    "len(coverdf[coverdf.isna().any(axis=1)])\n",
    "len(lyme_weather_cover[lyme_weather_cover.isna().any(axis=1)])\n",
    "\n",
    "lyme_weather_cover = lyme_weather_cover.dropna(axis=0, how='any')\n",
    "lyme_weather_cover.to_csv('./data/interim/lyme_weather_cover.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "census_data_urb_rur = pd.read_csv('./data/PctUrbanRural_County.csv')\n",
    "\n",
    "census_data = pd.read_csv('./data/raw/cc-est2017-alldata.csv', encoding = \"ISO-8859-1\")\n",
    "census_imp = census_data[['STATE', 'AGEGRP', 'COUNTY', 'STNAME', 'CTYNAME', 'YEAR', 'TOT_POP']]\n",
    "\n",
    "FIPS_POP = census_pop.groupby('FIPS')['TOT_POP'].sum().reset_index()\n",
    "\n",
    "census_tot = census_data_urb_rur.set_index(keys = ['STATE', 'COUNTY']).join(census_imp.set_index(keys=['STATE', 'COUNTY'])).reset_index()\n",
    "\n",
    "census_tot['State FIPS Code'] = census_tot['STATE'].apply(lambda x: str(x).zfill(2))\n",
    "census_tot['County FIPS Code'] = census_tot['COUNTY'].apply(lambda x: str(x).zfill(3))\n",
    "census_tot['FIPS'] = census_tot['State FIPS Code'] + census_tot['County FIPS Code']\n",
    "\n",
    "census_temp = census_tot[['FIPS', 'POP_URBAN', 'POPPCT_URBAN', 'AREA_URBAN', 'AREAPCT_URBAN',\n",
    "                  'POP_RURAL', 'POPPCT_RURAL', 'AREA_RURAL', 'AREAPCT_RURAL']]\n",
    "census_temp.drop_duplicates(subset=None, keep='first', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "inpdf = pd.read_csv('./data/lyme_weather_cover.csv')\n",
    "\n",
    "inpdf['Cases_new'] = inpdf.apply(lambda x: x.Cases*1.4 if x.YR > 2008 else x.Cases, axis=1)\n",
    "\n",
    "inpdf['long'] = inpdf['long_lat'].apply(lambda x: float(x[1:6]))\n",
    "inpdf['lat'] = inpdf['long_lat'].apply(lambda x: float(x.split(',')[1][:7]))\n",
    "fips = inpdf['FIPS']\n",
    "\n",
    "# load NY data and insert it into our dataframe  \n",
    "\n",
    "ny_data = pd.read_csv('/Users/avani/Dropbox/Work/dataScience/Insight/PredictLyme/data/nyc_all.csv')\n",
    "nydata = ny_data.dropna()\n",
    "\n",
    "inpdf['counties'] = inpdf.Ctyname.apply(lambda x: x.split('County')[0].strip().lower())\n",
    "nydata['counties'] = nydata.County.apply(lambda x: x.strip().lower())\n",
    "\n",
    "allindxs = []\n",
    "cases = []\n",
    "\n",
    "for county in nydata.counties:\n",
    "    indxs = inpdf.index[(inpdf.counties==county) & (inpdf.Stname=='New York')].to_list()\n",
    "    allindxs.append(indxs)\n",
    "    for indx in indxs:\n",
    "        tdf = nydata[nydata['counties']==county]\n",
    "        cases.append(tdf[str(int(inpdf.iloc[indx]['YR']))])\n",
    "\n",
    "allindxs_open = [item for sublist in allindxs for item in sublist]\n",
    "cases[583] = 510 # figure out whats happening \n",
    "for i in range(len(allindxs_open)):\n",
    "    ele = allindxs_open[i]\n",
    "    inpdf.at[ele,'Cases'] = cases[i]\n",
    "    \n",
    "inpdf['Stfips'] = inpdf['STCODE'].apply(lambda x: str(x).zfill(2))\n",
    "inpdf['Ctfips'] = inpdf['CTYCODE'].apply(lambda x: str(x).zfill(3))\n",
    "inpdf['FIPS'] = inpdf['Stfips'] + inpdf['Ctfips']\n",
    "\n",
    "inpdf = inpdf.set_index('FIPS').join(FIPS_POP.set_index('FIPS'))\n",
    "inpdf.reset_index()\n",
    "\n",
    "inpdf['FIPS'] = inpdf['Stfips'] + inpdf['Ctfips']\n",
    "\n",
    "inpdf = inpdf.set_index('FIPS').join(census_temp.set_index('FIPS'))\n",
    "\n",
    "inpdf['Cases_norm'] = inpdf['Cases']/inpdf['TOT_POP']\n",
    "\n",
    "inpdf['tavg']= inpdf[['JAN_TMPC', 'FEB_TMPC', 'MAR_TMPC', 'APR_TMPC', 'MAY_TMPC', 'JUN_TMPC', 'JUL_TMPC', 'AUG_TMPC', \n",
    "                     'SEP_TMPC', 'OCT_TMPC', 'NOV_TMPC', 'DEC_TMPC']].mean(axis=1)\n",
    "\n",
    "inpdf['tmax']= inpdf[['JAN_TMAX', 'FEB_TMAX', 'MAR_TMAX', 'APR_TMAX', 'MAY_TMAX', 'JUN_TMAX', 'JUL_TMAX', 'AUG_TMAX', \n",
    "                     'SEP_TMAX', 'OCT_TMAX', 'NOV_TMAX', 'DEC_TMAX']].mean(axis=1)\n",
    "\n",
    "inpdf['tmin']= inpdf[['JAN_TMIN', 'FEB_TMIN', 'MAR_TMIN', 'APR_TMIN', 'MAY_TMIN', 'JUN_TMIN', 'JUL_TMIN', 'AUG_TMIN', \n",
    "                     'SEP_TMIN', 'OCT_TMIN', 'NOV_TMIN', 'DEC_TMIN']].mean(axis=1)\n",
    "\n",
    "inpdf['pcpn']= inpdf[['JAN_PCPN', 'FEB_PCPN', 'MAR_PCPN', 'APR_PCPN', 'MAY_PCPN', 'JUN_PCPN', 'JUL_PCPN', 'AUG_PCPN', \n",
    "                     'SEP_PCPN', 'OCT_PCPN', 'NOV_PCPN', 'DEC_PCPN']].mean(axis=1)\n",
    "  \n",
    "inpdf.to_csv('./data/alldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting out features and states \n",
    "\n",
    "#imp_columns = ['Stname','TOT_POP', 'Cases','FIPS', 'CENSUSAREA','long','lat','YR', 'tavg', 'pcpn', 'tmax', 'tmin', 'Forest & Woodland', 'Shrub & Herb Vegetation']#\n",
    "               #'Desert & Semi-Desert', 'Introduced & Semi Natural Vegetation','Recently Disturbed or Modified','Open Water','Developed & Other Human Use']\n",
    "\n",
    "list_val_states = ['New York', 'Massachusetts', 'Pennsylvania', 'Connecticut', 'Michigan', 'Maine', 'New Hampshire', 'Vermont']\n",
    "\n",
    "val_df = inpdf[inpdf['Stname'].isin(list_val_states)]\n",
    "val_df.to_csv('./data/val_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the climate projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_temp = finaldf[finaldf.ST_new==1]['FIPS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "list_files = os.listdir('./data/CMIPS_data/')\n",
    "\n",
    "list_cols = ['YEAR','JAN_PCPN', 'FEB_PCPN', 'MAR_PCPN', 'APR_PCPN', 'MAY_PCPN', 'JUN_PCPN',\n",
    "       'JUL_PCPN', 'AUG_PCPN', 'SEP_PCPN', 'OCT_PCPN', 'NOV_PCPN', 'DEC_PCPN',\n",
    "       'JAN_TMPC', 'FEB_TMPC', 'MAR_TMPC', 'APR_TMPC', 'MAY_TMPC',\n",
    "       'JUN_TMPC', 'JUL_TMPC', 'AUG_TMPC', 'SEP_TMPC', 'OCT_TMPC', 'NOV_TMPC',\n",
    "       'DEC_TMPC', 'JAN_TMIN', 'FEB_TMIN', 'MAR_TMIN', 'APR_TMIN', 'MAY_TMIN',\n",
    "       'JUN_TMIN', 'JUL_TMIN', 'AUG_TMIN', 'SEP_TMIN', 'OCT_TMIN', 'NOV_TMIN',\n",
    "       'DEC_TMIN', 'JAN_TMAX', 'FEB_TMAX', 'MAR_TMAX', 'APR_TMAX', 'MAY_TMAX',\n",
    "       'JUN_TMAX', 'JUL_TMAX', 'AUG_TMAX', 'SEP_TMAX', 'OCT_TMAX', 'NOV_TMAX',\n",
    "       'DEC_TMAX','FIPS']\n",
    "\n",
    "projectiondf =  pd.DataFrame(columns=list_cols)\n",
    "\n",
    "for file in list_files:\n",
    "    \n",
    "    clim = pd.read_csv('./data/CMIPS_data/'+file, skiprows=1, nrows=1440)\n",
    "\n",
    "    nfips = int((len(clim.columns)-1)/4)\n",
    "    clim = clim[clim.columns[0:nfips+1]]\n",
    "    fips = clim.columns[1:]\n",
    "\n",
    "    prpn_df, tavg_df, tmax_df, tmin_df = clim.iloc[1:361,:],clim.iloc[364:724,:],clim.iloc[727:1087,:],clim.iloc[1090:,:]\n",
    "\n",
    "    prpn_df['year'] = prpn_df['Unnamed: 0'].apply(lambda x: int(x[0:4]))    \n",
    "    tavg_df['year'] = tavg_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "    tmin_df['year'] = tmin_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "    tmax_df['year'] = tmax_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "\n",
    "    prpn_df = prpn_df[(prpn_df['year']>=2017) & (prpn_df['year']<=2025)]\n",
    "    tavg_df = tavg_df[(tavg_df['year']>=2017) & (tavg_df['year']<=2025)]\n",
    "    tmin_df = tmin_df[(tmin_df['year']>=2017) & (tmin_df['year']<=2025)]\n",
    "    tmax_df = tmax_df[(tmax_df['year']>=2017) & (tmax_df['year']<=2025)]\n",
    "\n",
    "    prpn_df['month'] = prpn_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7]))    \n",
    "    tavg_df['month'] = tavg_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "    tmin_df['month'] = tmin_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "    tmax_df['month'] = tmax_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "\n",
    "    prpn_df['month'] = prpn_df['month'] + '_PCPN'\n",
    "    tmax_df['month'] = tmax_df['month'] + '_TMAX'\n",
    "    tmin_df['month'] = tmin_df['month'] + '_TMIN'\n",
    "    tavg_df['month'] = tavg_df['month'] + '_TMPC'\n",
    "\n",
    "    for year in prpn_df.year.unique():\n",
    "\n",
    "        temp = prpn_df[prpn_df['year']==year]\n",
    "        temp0 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp0['FIPS']  = temp0.index\n",
    "        temp0 = temp0.reset_index(drop=True)\n",
    "\n",
    "        temp = tmax_df[tmax_df['year']==year]\n",
    "        temp1 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp1['FIPS']  = temp1.index\n",
    "        temp1 = temp1.reset_index(drop=True)\n",
    "\n",
    "        temp = tavg_df[tavg_df['year']==year]\n",
    "        temp2 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp2['FIPS']  = temp2.index\n",
    "        temp2 = temp2.reset_index(drop=True)\n",
    "\n",
    "        temp = tmin_df[tmin_df['year']==year]\n",
    "        temp3 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp3['FIPS']  = temp3.index\n",
    "        temp3 = temp3.reset_index(drop=True)\n",
    "\n",
    "        merged = temp1.set_index('FIPS').join(temp0.set_index('FIPS')).join(temp2.set_index('FIPS')).join(temp3.set_index('FIPS'))\n",
    "        merged['YEAR'] = year\n",
    "        merged['FIPS'] = merged.index\n",
    "        \n",
    "        projectiondf = projectiondf.append(merged)\n",
    "\n",
    "projectiondf['FIPS'] = projectiondf.index\n",
    "projectiondf.to_csv('./climate_projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/avani/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "list_files = ['CMIPS_ct27.csv',\n",
    " 'CMIPS_ct33.csv',\n",
    " 'CMIPS_ct26.csv',\n",
    " 'CMIPS_ct54_55_56.csv',\n",
    " 'CMIPS_ct36.csv',\n",
    " 'CMIPS_ct37.csv',\n",
    " 'CMIPS_ct45_46.csv',\n",
    " 'CMIPS_ct16_18.csv',\n",
    " 'CMIPS_ct145.csv',\n",
    " 'CMIPS_ct28_32.csv',\n",
    " 'CMIPS_ct54_55_56 (1).csv',\n",
    " 'CMIPS_ct45_46 (1).csv',\n",
    " 'CMIPS_ct34_35.csv',\n",
    " 'CMIPS_ct51.csv',\n",
    " 'CMIPS_ct19_25.csv',\n",
    " 'CMIPS_ct46_50.csv',\n",
    " 'CMIPS_ct19_24.csv',\n",
    " 'CMIPS_ct38_39.csv',\n",
    " 'CMIPS_ct10_13.csv',\n",
    " 'CMIPS_ct89.csv',\n",
    " 'CMIPS_ct19_22.csv',\n",
    " 'CMIPS_ct40_42.csv',\n",
    " 'CMIPS_ct6.csv']\n",
    "\n",
    "list_cols = ['YEAR','JAN_PCPN', 'FEB_PCPN', 'MAR_PCPN', 'APR_PCPN', 'MAY_PCPN', 'JUN_PCPN',\n",
    "       'JUL_PCPN', 'AUG_PCPN', 'SEP_PCPN', 'OCT_PCPN', 'NOV_PCPN', 'DEC_PCPN',\n",
    "       'JAN_TMPC', 'FEB_TMPC', 'MAR_TMPC', 'APR_TMPC', 'MAY_TMPC',\n",
    "       'JUN_TMPC', 'JUL_TMPC', 'AUG_TMPC', 'SEP_TMPC', 'OCT_TMPC', 'NOV_TMPC',\n",
    "       'DEC_TMPC', 'JAN_TMIN', 'FEB_TMIN', 'MAR_TMIN', 'APR_TMIN', 'MAY_TMIN',\n",
    "       'JUN_TMIN', 'JUL_TMIN', 'AUG_TMIN', 'SEP_TMIN', 'OCT_TMIN', 'NOV_TMIN',\n",
    "       'DEC_TMIN', 'JAN_TMAX', 'FEB_TMAX', 'MAR_TMAX', 'APR_TMAX', 'MAY_TMAX',\n",
    "       'JUN_TMAX', 'JUL_TMAX', 'AUG_TMAX', 'SEP_TMAX', 'OCT_TMAX', 'NOV_TMAX',\n",
    "       'DEC_TMAX','FIPS']\n",
    "\n",
    "projectiondf =  pd.DataFrame(columns=list_cols)\n",
    "\n",
    "for file in list_files:\n",
    "    \n",
    "    clim = pd.read_csv('./data/raw/CMIPS_data/'+file, skiprows=1, nrows=1440)\n",
    "\n",
    "    nfips = int((len(clim.columns)-1)/4)\n",
    "    clim = clim[clim.columns[0:nfips+1]]\n",
    "    fips = clim.columns[1:]\n",
    "\n",
    "    prpn_df, tavg_df, tmax_df, tmin_df = clim.iloc[1:361,:],clim.iloc[364:724,:],clim.iloc[727:1087,:],clim.iloc[1090:,:]\n",
    "\n",
    "    prpn_df['year'] = prpn_df['Unnamed: 0'].apply(lambda x: int(x[0:4]))    \n",
    "    tavg_df['year'] = tavg_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "    tmin_df['year'] = tmin_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "    tmax_df['year'] = tmax_df['Unnamed: 0'].apply(lambda x: int(x[0:4])) \n",
    "\n",
    "    prpn_df = prpn_df[(prpn_df['year']>=2000) & (prpn_df['year']<=2025)]\n",
    "    tavg_df = tavg_df[(tavg_df['year']>=2000) & (tavg_df['year']<=2025)]\n",
    "    tmin_df = tmin_df[(tmin_df['year']>=2000) & (tmin_df['year']<=2025)]\n",
    "    tmax_df = tmax_df[(tmax_df['year']>=2000) & (tmax_df['year']<=2025)]\n",
    "\n",
    "    prpn_df['month'] = prpn_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7]))    \n",
    "    tavg_df['month'] = tavg_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "    tmin_df['month'] = tmin_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "    tmax_df['month'] = tmax_df['Unnamed: 0'].apply(lambda x: month_map(x[5:7])) \n",
    "\n",
    "    prpn_df['month'] = prpn_df['month'] + '_PCPN'\n",
    "    tmax_df['month'] = tmax_df['month'] + '_TMAX'\n",
    "    tmin_df['month'] = tmin_df['month'] + '_TMIN'\n",
    "    tavg_df['month'] = tavg_df['month'] + '_TMPC'\n",
    "\n",
    "    for year in prpn_df.year.unique():\n",
    "\n",
    "        temp = prpn_df[prpn_df['year']==year]\n",
    "        temp0 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp0['FIPS']  = temp0.index\n",
    "        temp0 = temp0.reset_index(drop=True)\n",
    "\n",
    "        temp = tmax_df[tmax_df['year']==year]\n",
    "        temp1 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp1['FIPS']  = temp1.index\n",
    "        temp1 = temp1.reset_index(drop=True)\n",
    "\n",
    "        temp = tavg_df[tavg_df['year']==year]\n",
    "        temp2 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp2['FIPS']  = temp2.index\n",
    "        temp2 = temp2.reset_index(drop=True)\n",
    "\n",
    "        temp = tmin_df[tmin_df['year']==year]\n",
    "        temp3 = temp.drop(labels=['Unnamed: 0', 'year'], axis=1).set_index(keys='month').transpose()\n",
    "        temp3['FIPS']  = temp3.index\n",
    "        temp3 = temp3.reset_index(drop=True)\n",
    "\n",
    "        merged = temp1.set_index('FIPS').join(temp0.set_index('FIPS')).join(temp2.set_index('FIPS')).join(temp3.set_index('FIPS'))\n",
    "        merged['YEAR'] = year\n",
    "        merged['FIPS'] = merged.index\n",
    "        \n",
    "        projectiondf = projectiondf.append(merged)\n",
    "\n",
    "projectiondf['FIPS'] = projectiondf.index\n",
    "projectiondf.to_csv('./climate_projections_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
